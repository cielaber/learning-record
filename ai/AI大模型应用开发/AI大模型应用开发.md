# AI的发展历史

![屏幕截图 2025-02-07 110842](.\image-ai\屏幕截图 2025-02-07 110842.png)

![屏幕截图 2025-02-07 110950](.\image-ai\屏幕截图 2025-02-07 110950.png)

# 注意力机制

## 注意力机制的意义

### RNN

循环神经网络（Rerrent Neural Network, RNN），RNN对具有序列特性的数据非常有效，它能挖掘数据中的时序信息以及语义信息，利用了RNN的这种能力，使深度学习模型在解决语音识别、语言模型、机器翻译以及时序分析等NLP领域的问题时有所突破。

参考学习：https://zhuanlan.zhihu.com/p/123211148

### 注意力机制的特点和优势

1. 注意力机制有助于克服循环神经网络（RNNs）的一些挑战，例如输入序列长度增加时性能下降和顺序处理输入导致的计算效率低下。
2. 在自然语言处理（NLP）、计算机视觉（Computer Vision）、跨模态任务和推荐系统等多个领域中，注意力机制已成为多项任务中的最先进模型，取得了显著的性能提升。
3. 注意力机制不仅可以提高主要任务的性能，还具有其他优势。它们被广泛用于提高神经网络的可解释性，帮助解释模型的决策过程，使得原本被认为是黑盒模型的神经网络变得更易解释。这对于人们对机器学习模型的公平性、可追溯性和透明度的关注具有重要意义。

#### 解决RNNs面临的挑战

1. 解决传统编码器-解码器模型的挑战，避免信息损失和无法建模输入输出对齐的问题。
2. 允许解码器访问整个编码的输入序列，通过注意力权重选择性地关注相关信息。
3. 自动学习注意力权重，捕捉编码器和解码器之间的相关性。
4. 构建上下文向量，使解码器能够全面访问输入序列并重点关注相关部分。
5. 提高模型性能，改善输出质量，并提供更好的解释性。

## 注意力机制的原理

注意力机制最早被使用在机器翻译的场景中，以下使用机器翻译作为例子。

### Encoder-Decoder架构

Encoder-Decoder架构是一种深度学习模型结构，广泛应用于自然语言处理（NLP）、图像处理、语音识别等领域。它主要由两部分组成：编码器（Encoder）和解码器（Decoder）。

#### 编码器（Encoder）

编码器的作用是接收输入序列，并将其转换成固定长度的[上下文向量](https://zhida.zhihu.com/search?content_id=240538428&content_type=Article&match_order=1&q=上下文向量&zhida_source=entity)（context vector）。这个向量是输入序列的一种内部表示，捕获了输入信息的关键特征。在自然语言处理的应用中，输入序列通常是一系列词语或字符。

#### 解码器（Decoder）

解码器的目标是将编码器产生的上下文向量转换为输出序列。在开始解码过程时，它首先接收到编码器生成的上下文向量，然后基于这个向量生成输出序列的第一个元素。接下来，它将自己之前的输出作为下一步的输入，逐步生成整个输出序列。

#### 注意力机制原理

![1738899766580](.\image-ai\1738899766580.png)

上面一幅图是传统的RNN Encoder-Decoder架构，在Encoder中将所有的输入转换成一个上下文向量，即Hidden State #3。

增加了注意力机制之后就如下图，它将所有Hidden State都传入到Decoder中。

![屏幕截图 2025-02-07 114341](.\image-ai\屏幕截图 2025-02-07 114341.png)

如图所示，解码器中能接受到所有输入侧的信息，并且可以根据权重设置不同h的重要程度，例如矩阵中的αij即表示机器翻译中各个输入词和译词之间的权重关系。

![屏幕截图 2025-02-07 115834](.\image-ai\屏幕截图 2025-02-07 115834.png)

注意力机制带来了一个很重要的思想：它用一个网络解决了机器翻译中翻译和对齐问题。在以往的NLP中都是一个模型做一个任务。后续大语言模型都采用了这样的思想。

![屏幕截图 2025-02-07 134834](.\image-ai\屏幕截图 2025-02-07 134834.png)

![屏幕截图 2025-02-07 134255](.\image-ai\屏幕截图 2025-02-07 134255.png)

注意力机制中通过alignment function解决了对齐问题。对齐函数的核心是找到输入和输出之间的关系，学习对齐函数的过程是有很多方式的。

比如参数化的方式，提供某一个函数，函数中存在一些超参数，可以提供一些数据去学习出来这些参数，然后学出来的对齐函数就是符合输入这些数据的函数。

比如非参数化的方式，直接提供一个核函数，比如一些机器视觉检测，通过一些经验总结出来了一些规律。

![屏幕截图 2025-02-07 135844](.\image-ai\屏幕截图 2025-02-07 135844.png)

![屏幕截图 2025-02-07 141138](.\image-ai\屏幕截图 2025-02-07 141138.png)

## Transformer

在机器翻译中，采用Encoder-Decoder架构，通过注意力机制解决了输入与输出对齐的问题。

![屏幕截图 2025-02-07 141808](.\image-ai\屏幕截图 2025-02-07 141808.png)

Transformer在其基础上做了改进，它改进了网络结构，并改变了对齐函数。将原来的基于序列对齐的RNN（seq-aligned RNNS）变为了self-attention。

机器翻译中注意力机制的RNN解决了词的翻译问题，self-attention解决的不是翻译问题，而是语义理解能力。

![屏幕截图 2025-02-07 141837](.\image-ai\屏幕截图 2025-02-07 141837.png)

![屏幕截图 2025-02-07 144022](.\image-ai\屏幕截图 2025-02-07 144022.png)



Transformer中的自注意力机制被扩展为多个注意力头，每个头可以学习不同的注意权重，以更好地捕捉不同类型的关系。多头注意力允许模型并行处理不同的信息子空间。![屏幕截图 2025-02-07 150019](.\image-ai\屏幕截图 2025-02-07 150019.png)

![屏幕截图 2025-02-07 150127](.\image-ai\屏幕截图 2025-02-07 150127.png)

本质上Transformer还是一个Encoder-Decoder结构。

![屏幕截图 2025-02-07 150922](.\image-ai\屏幕截图 2025-02-07 150922.png)

![屏幕截图 2025-02-07 151133](.\image-ai\屏幕截图 2025-02-07 151133.png)

![屏幕截图 2025-02-07 151430](.\image-ai\屏幕截图 2025-02-07 151430.png)

### Transformer的重要性

Transformer抛弃了RNN的结构，没有了RNN的缺点。

Transformer是面向GPU并行计算友好的，由于之前的RNN是有序列的，前后依赖的，但Transformer的Decoder没有依赖关系，训练效率更高，能训练更大的模型。

Transformer通过位置编码，能够更好的捕获语句之间的依赖关系，能更好的理解语义。

#### 开启了一个基础模型的时代

在以往，如果需要做一个问答模型，需要大量的数据训练很多的模型，不同的下游任务都需要做不同的训练。

但是有了Transformer之后，只需要用大量足够多的数据训练出基础模型，然后下游的任务只需要做轻量的适配就能做出不同的机器人。

就像一个人的学习在小学初中学习了很多通用的知识之后，有了一定的理解能力，后面在不同的岗位上只需要学习这个岗位少量独有的知识就行。

![屏幕截图 2025-02-07 152018](.\image-ai\屏幕截图 2025-02-07 152018.png)

# 大模型

![屏幕截图 2025-02-07 153517](.\image-ai\屏幕截图 2025-02-07 153517.png)

LSTM：在深度学习领域，循环神经网络（RNN）在处理序列数据方面具有独特的优势，例如语音识别、自然语言处理等任务。然而，传统的 RNN 在处理长序列数据时面临着严重的梯度消失问题，这使得网络难以学习到长距离的依赖关系。LSTM 作为一种特殊的 RNN 架构应运而生，有效地解决了这一难题，成为了序列建模领域的重要工具。

Word2vec：Word2Vec是google在2013年推出的一个NLP工具，它的特点是能够将单词转化为向量来表示，这样词与词之间就可以定量的去度量他们之间的关系，挖掘词之间的联系。

GloVe：它是一个基于全局词频统计的词表征工具，它可以把一个单词表达成一个由实数组成的向量，这些向量捕捉到了单词之间一些语义特性，比如相似性、类比性等。我们通过对向量的运算，比如欧几里得距离或者cosine相似度，可以计算出两个单词之间的语义相似性。

Word Embedding（词嵌入）：词嵌入技术是自然语言处理（NLP）领域的一项重大创新，它极大地推动了计算机理解和处理人类语言的能力。通过将单词、句子甚至图像转换为数字向量，词嵌入技术不仅改善了文本的表示方式，更重要的是，它捕捉到了语言的本质和丰富的语义信息。Word2vec和GloVe都是Word Embedding。

## 神经网络

![屏幕截图 2025-02-07 155051](.\image-ai\屏幕截图 2025-02-07 155051.png)

## BERT

BERT提出了预训练+微调的模式，上游通过大量未标注的数据训练出基础模型，下游通过少量标注数据进行微调。

![屏幕截图 2025-02-07 160410](.\image-ai\屏幕截图 2025-02-07 160410.png)

原来的模型都是单向的理解输入的语句，但是BERT认为语言的理解是双向的，并且BERT尝试双向的学习是否会带来更大的价值，因此BERT采用双向理解的方式。

![屏幕截图 2025-02-07 183130](.\image-ai\屏幕截图 2025-02-07 183130.png)

最终BERT在一些基准测试中取得了很好的成绩。

### Masked LM

Masked Language Model是一种无监督学习任务，旨在预测被遮蔽或掩码的语言表示。在BERT中，Masked LM任务的目标是预测被随机掩码的单词，同时利用上下文信息来恢复这些单词。通过这种方式，BERT能够双向理解语言上下文。

### BERT的价值

1. **全方位上下文理解**：与以前的模型（例如GPT）相比，BERT能够双向理解上下文，即同时考虑一个词的左边和右边的上下文。这种全方位的上下文理解使得BERT能够更好地理解语言，特别是在理解词义、消歧等复杂任务上有明显优势。
2. **预训练+微调（Pre-training + Fine-tuning）的策略**：BERT模型先在大规模无标签文本数据上进行预训练，学习语言的一般性模式，然后在具体任务的标签数据上进行微调。这种策略让BERT能够在少量标签数据上取得很好的效果，大大提高了在各种NLP任务上的表现。
3. **跨任务泛化能力**：BERT通过微调可以应用到多种NLP任务中，包括但不限于文本分类、命名实体识别、问答系统、情感分析等。它的出现极大地简化了复杂的NLP任务，使得只需一种模型就能处理多种任务。
4. **多语言支持**：BERT提供了多语言版本（Multilingual BERT），可以支持多种语言，包括但不限于英语、中文、德语、法语等，使得NLP任务能够覆盖更广的语言和区域。
5. **性能优异**：自BERT模型提出以来，它在多项NLP基准测试中取得了优异的成绩，甚至超过了人类的表现。它的出现标志着NLP领域进入了预训练模型的新时代。
6. **开源和可接入性**：BERT模型和预训练权重由Google公开发布，让更多的研究者和开发者可以利用BERT模型进行相关研究和应用开发，推动了整个NLP领域的发展。

### BERT与GPT的差别

![屏幕截图 2025-02-07 183516](.\image-ai\屏幕截图 2025-02-07 183516.png)

![屏幕截图 2025-02-07 183538](.\image-ai\屏幕截图 2025-02-07 183538.png)

## GPT

### NLP语言模型发展

![屏幕截图 2025-02-08 133209](.\image-ai\屏幕截图 2025-02-08 133209.png)

**人工规则阶段**

根据已有的经验和知识定制规则集，并产生一些系统。能够

缺点：

- 规则集的制作成本很高。
- 规则集的质量受限于专家的水平。
- 由于规则不够通用，只能处理特定规则的任务，比如垃圾邮件识别。

**统计机器学习阶段**

根据已有的数据进行数学层面的统计学习，学习出数据上的数学规律，比如对某类文章进行词频统计，能够知道这类文章的词符合什么类型的概率分布。

缺点：

- 需要让机器识别到词的含义，因此需要对数据进行标注，大量的数据需要一定的标注成本。
- 受限于标注成本，只能生产某些特定的小范围内的任务模型，例如特定类型的小说和新闻。

**深度学习阶段**

计算机视觉领域出现突破性进展，算力、各种类型的框架和工具开始涌现，更多的数据能带来更好的结果这件事情在机器视觉领域得到验证，优秀的网络结构和经验也在NLP方向得以借鉴。

但此时的深度学习依旧是只能在默写特定的领域，比如机器翻译，也只能是某个特定语言到另一个特定语言的翻译，依旧是需要标注数据引导机器进行学习。

**预训练阶段**

Pre-training + Fine-tuning模式让机器学习足够多的未标注数据，让其产生语言理解能力，下游任务只需要对其进行微调。

这个阶段虽然模型已经有很好的理解能力，但是回答能力不够优秀。

**大语言模型阶段**

大语言模型在技术上依旧是属于预训练的范畴，但和预训练阶段的区别在于：

- 它的模型规模参数量有非常大的提升，导致模型能力有更大的提升。
- 训练模型技巧更加丰富，可以进行指令微调，进行提示词学习，或者基于人类的反馈进行训练，使其能够更加准确的回答问题。

### 从GPT-1到GPT-3

**预训练模型**

![屏幕截图 2025-02-08 142033](.\image-ai\屏幕截图 2025-02-08 142033.png)

**预训练模型的三种网络架构（2018-2020）**

![屏幕截图 2025-02-08 142057](.\image-ai\屏幕截图 2025-02-08 142057.png)

![屏幕截图 2025-02-08 142133](.\image-ai\屏幕截图 2025-02-08 142133.png)

GPT-1在原有的Transformer结构上去掉了Encoder结构，并把Decoder加到了12层，然后在特定任务中进行微调，这个阶段GPT在技术上面并没有太大的创新，所以它当时的名称叫**Generative Pretrained Transformer (GPT-1)**。

![屏幕截图 2025-02-08 142615](.\image-ai\屏幕截图 2025-02-08 142615.png)

虽然技术上没有太多的创新，但是只用Decoder的思路加上一个初创公司用较少的成本带来了很好的效果，这引发了当时的一些关注。

在看到较好的成果之后，GPT-2在GPT-1的基础上用了更多的参数和更多的数据。

而且GPT-2有一个很重要的思想，尝试将数据增加到足够多时，不需要对下游的各个任务进行Fine-tuning。

GPT-3在原来的基础上增加了更多的参数和更多的数据，使用爬虫增加了互联网的数据。并且GPT-3提出了一个很重要的概念：**In-Context Learning**。

**In-Context Learning**

在上下文中学习指的是大型语言模型的一种能力，即在给定的上下文中使用新的输入来改善模型的输出。这种学习方式并不涉及到梯度更新或微调模型的参数，而是通过提供一些具有特定格式或结构的示例输入，使模型能够在生成输出时利用这些信息。例如，如果你在对话中包含一些英法翻译的例子，然后问模型一个新的翻译问题，模型可能会根据你提供的上下文示例生成正确的翻译。

在之前的模型训练中，针对模型进行Fine-tuning时，需要改变模型，当模型变得足够大时，修改模型是一件很困难的事情。因此GPT-3提出在In-Context Learning，在上下文语境中学习，而不需要改变模型参数，也就是我们告诉大模型怎么和我们交流，也就是提示词工程的来源。

![屏幕截图 2025-02-08 145712](.\image-ai\屏幕截图 2025-02-08 145712.png)

### GPT赢在哪？

在 GPT 模型的演进过程中，OpenAI 采用了一系列的训练策略，这包括基础的**大规模预训练**，也包括后续的**指令微调**等方法。这两种策略在模型的训练过程中起到了不同的作用。

• **预训练(Pre-Trained)**：大规模预训练是为了使模型获取丰富的语言知识和理解能力。在预训练过程中，模型通过大量的无标签数据来学习语言的基础知识，这一过程主要是依赖无监督学习的。

• **指令微调(Instruction-Tuning)**：在预训练模型的基础上，通过针对特定任务的标注数据进行微调，能够使模型在特定任务上的表现得到提升。同时，通过对微调数据的精心设计和选择，还能够引导模型按照人类的预期来执行任务。这一过程主要依赖有监督学习。

在这个过程中，预训练和微调是相辅相成的。预训练为模型提供了丰富的语言知识，而微调则利用这些知识来解决特定的任务。然而，微调的数据量通常比预训练的数据量要少得多，因此微调的主要作用并不是为模型注入新的知识，而是激发和引导模型利用已有的知识来完成特定任务。

除了**大规模预训练**和**指令微调**之外，在GPT-3.5中还加入了**奖励机制和人类反馈机制**。

**初代GPT-3.5 : code-davinci-002**

![屏幕截图 2025-02-08 153320](.\image-ai\屏幕截图 2025-02-08 153320.png)

初代GPT-3.5系列（以下简称新模型）相比 GPT-3 系列模型，具有以下优点：

• **人类指令响应 (Responding to Human Instructions)**：新模型能针对特定指令生成更恰当的回应，而非回复训练集中频繁出现的无关句子。

• **任务泛化能力 (Task Generalization)**：当新模型接收大量指令调整后，能自动适应并有效回答未见过的新指令，这在应对用户不断变化的问题上至关重要。

• **代码理解与生成 (Code Understanding and Generation)**：经过代码训练的新模型能理解并生成代码，强化了编程相关应用的能力。

• **复杂推理的思维链(Chain of Thought for Complex Reasoning)**：新模型已提高思维链推理能力，使其能处理需要多步推理的问题，这可能是突破模型缩放法则(scaling laws)限制，实现真正的突现性能力的关键。

在基础模型 code-davinci-002 上指令微调后得到了 text-davinci-002 。它在以下数据作了微调：（一）人工标注的指令和期待的输出；（二）由人工标注者选择的模型输出。

**ChatGPT的三段训练法**

![屏幕截图 2025-02-08 153107](.\image-ai\屏幕截图 2025-02-08 153107.png)

### GPT-4

2022年8月，GPT-4 模型训练完成。2023年3月14日，OpenAI 正式发布 GPT-4。 与GPT-3和GPT-3.5相比，GPT-4在各方面都有所优化和提升：

1. **多模态模型** ：GPT-4支持图像输入，出色的视觉信息理解能力使得GPT-4能对接更多样化的下游任务，如：描述不寻常图像中的幽默、总结截屏文本以及回答包含图表的试题。在文本理解能力上，GPT-4 在中文和多轮对话中也表现出远超 GPT-3.5 的能力。
2. **扩展上下文窗口**：gpt-4 and gpt-4-32k 分别提供了最大长度为8192和32768个token的上下文窗口。这使得 GPT-4可以通过更多的上下文来完成更复杂的任务，也为 思维链（CoT）、思维树（ToT）等后续工作提供了可能。
3. **GPT+生态** ：借助GPT-4强大能力，依托 ChatGPT Plugin 搭建AIGC应用生态商店（类似 App Store）。
4. **应用+GPT** ：GPT-4已经被应用在多个领域，包括微软Office、Duolingo、Khan Academy等。

# 提示学习

## Prompt Learning

### Prompt Learning vs In-context Learning

**Prompt learning**

是一种使用预训练语言模型的方法，它不会修改模型的权重。在这种方法中，模型被给予一个提示（prompt），这个提示是模型输入的一部分，它指导模型产生特定类型的输出。这个过程不涉及到对模型权重的修改，而是利用了模型在预训练阶段学习到的知识和能力。

**In-context learning** 

是指模型在处理一系列输入时，使用前面的输入和输出作为后续输入的上下文。这是Transformer模型（如GPT系列）的一种基本特性。例如，当模型在处理一个对话任务时，它会使用对话中的前几轮内容作为上下文，来生成下一轮的回答。这个过程也不涉及到对模型权重的修改。

**区别**

prompt learning和in-context learning都是利用预训练语言模型的方法，它们都不会修改模型的权重。它们的主要区别在于，prompt learning关注的是如何通过设计有效的提示来**引导模型的输出**，而in-context learning则关注的是如何**利用输入序列中的上下文信息**来影响模型的输出。

### Prompt Learning vs Prompt Tuning

Prompt learning和prompt tuning都是自然语言处理（NLP）中的概念，它们都与如何使用和优化预训练语言模型（例如GPT-3或GPT-4）有关。

**Prompt learning**：是一种方法，其中模型**被训练以响应特定的提示（prompt）**。在这种情况下，提示是模型输入的一部分，它指导模型产生特定类型的输出。例如，如果你向模型提供了"Translate the following English text to French: {text}"这样的提示，模型就会学习到这是一个翻译任务，并尝试将{text}从英语翻译成法语。这种方法的关键在于找到能够引导模型正确响应的有效提示。

**Prompt tuning**，又称为"prompt engineering"，是一种**优化技术**，它涉及到寻找或生成能够最大限度提高模型性能的提示。这可能涉及到使用启发式方法、人工智能搜索算法，或者甚至是人工选择和优化提示。Prompt tuning的目标是找到一种方式，使得当给定这个提示时，模型能够生成最准确、最相关的输出。

**区别**

prompt learning和prompt tuning都与如何使用和优化模型的输入提示有关。它们的主要区别在于，prompt learning更关注于如何训练模型以响应特定的提示，而prompt tuning则更关注于如何找到或生成最优的提示以提高模型的性能。

## 思维链

思维链（Chain of Thought，CoT）是一种人工智能技术，提升大型语言模型在复杂推理任务中的表现。通过在模型的输入和输出之间插入一系列逻辑推理步骤，帮助模型逐步分析和解决问题。与传统的直接从问题到答案的提示方法不同，CoT强调在得出结论前展示详细的思考过程，使模型能够更好地理解和处理需要多步骤逻辑推理的问题，如算术推理、常识推理和符号推理等。不仅增强了模型的推理能力，还提高了其输出的可解释性。

**Chain-of-Thought Prompting**作为一种促进语言模型推理的方法具有几个吸引人的特点：

- 首先，从原则上讲，CoT 允许模型将多步问题分解为中间步骤，这意味着可以将额外计算资源分配给需要更多推理步骤的问题。
- 其次，CoT 提供了对模型行为的可解释窗口，提示了它可能是如何得出特定答案的，并提供了调试推理路径错误之处的机会（尽管完全描述支持答案的模型计算仍然是一个未解决问题）。
- 第三，在数学应用题、常识推理和符号操作等任务中都可以使用思维链推理（CoT Reasoning），并且在原则上适用于任何人类能够通过语言解决的任务。
- 最后，在足够大规模现成语言模型中很容易引发 CoT Reasoning ，只需在少样本提示示例中包含一些连贯思路序列即可。

**案例**

![屏幕截图 2025-02-11 133617](.\image-ai\屏幕截图 2025-02-11 133617.png)
    ![屏幕截图 2025-02-11 133702](.\image-ai\屏幕截图 2025-02-11 133702.png)



![屏幕截图 2025-02-11 133111](.\image-ai\屏幕截图 2025-02-11 133111.png)

**CoT实验结论**

1. 对于小模型来说，CoT Prompting 无法带来性能提升，甚至可能带来性能的下降。
2. 对于大模型来说，CoT Prompting 涌现出了性能提升。
3. 对于复杂的问题，CoT Prompting 能获得更多的性能收益。

![屏幕截图 2025-02-11 132547](.\image-ai\屏幕截图 2025-02-11 132547.png)

## 自洽性（Self-Consistency）：多路径推理

自洽性（Self-Consistency）是指在推理过程中，多个推理路径产生的一致性结果能够相互验证，形成一种稳定且可靠的决策。通过生成多个推理路径，模型不仅能够在面对复杂问题时更准确地推导出结论，还能在面对不确定性时，依靠多个推理路径的“共识”来减少错误率。

具体而言，自洽性方法引导模型探索多个推理途径，并评估这些途径之间的一致性。通过整合这些不同路径的推理结果，模型最终得出一个可信度较高的结论。这种方法特别适合用于那些依赖多步推理、含有噪声或需要复杂判断的任务。

![屏幕截图 2025-02-11 135544](.\image-ai\屏幕截图 2025-02-11 135544.png)

### 关于 CoT与大模型逻辑推理能力的现状

通过思维链，我们可以看到大语言模型的强与弱：

- 它强在，模型规模的提高，让语义理解、符号映射、连贯文本生成等能力跃升，从而让多步骤推理的思维链成为可能，带来“智能涌现” 。
- 它弱在，即使大语言模型表现出了前所未有的能力，但思维链暴露了它，依然是鹦鹉学舌，而非真的产生了意识。

没有思维链，大模型几乎无法实现逻辑推理。

但有了思维链，大语言模型也可能出现错误推理，尤其是非常简单的计算错误。Jason Wei 等的论文中，曾展示过在 GSM8K 的一个子集中，大语言模型出现了 8% 的计算错误，比如6 * 13 = 68（正确答案是78）。

![屏幕截图 2025-02-11 140627](.\image-ai\屏幕截图 2025-02-11 140627.png)

目前的大语言模型暂时还是在解决系统一的问题，系统二发生的科学解释人类自身都没有搞明白。

## 思维树（Tree-of-Thoughts, ToT）

语言模型越来越多地被用于广泛的问题解决任务，但在推理过程中仍然局限于基于标记的、从左到右的决策过程。这意味着它们在需要探索、战略前瞻或初始决策起关键作用的任务中可能会有所不足。为了克服这些挑战，引入了一种新的语言模型推理框架，称为“思维树”（ToT），它推广了流行的“思维链”方法，可以对文本的连贯单元（“思维”）进行探索，作为问题解决的中间步骤。ToT允许语言模型通过考虑多种不同的推理路径和自我评估选择来进行有意的决策，并在必要时向前或向后进行全局选择。

人类问题解决的研究表明，人们通过组合问题空间进行搜索——树形结构，其中节点表示部分解决方案，分支对应修改它们的操作。选择哪个分支是由启发式决定的，这些启发式帮助导航问题空间，并指导问题解决者朝着解决方案前进。这个观点突显了使用语言模型解决一般问题的现有方法存在两个关键不足之处：

- 在局部上，它们不探索思考过程中的不同延伸——树的分支。
- 在全局上，它们不包含任何类型的规划、前瞻或回溯，以帮助评估这些不同的选项——这似乎是人类问题解决的特征之一。

为了解决这些不足，引入了思维树（Tree of Thoughts，ToT），这是一种允许语言模型在思考上探索多个推理路径的范 paradigm。ToT将任何问题框架化为对树的搜索，其中每个节点都是表示部分解决方案的状态s = [x, z1···i]，其中包含输入和到目前为止的思考序列。ToT的具体实例包括回答以下四个问题：

- **思维分解**：如何将中间过程分解为思考步骤；
- **思维生成**：如何从每个状态生成潜在的思考；
- **状态评估**：如何启发性地评估状态；
- **搜索算法**：使用什么搜索算法。

# 大模型开发基础：OpenAI Embedding

## 计算机的数据表示

**图像表示**

![屏幕截图 2025-02-11 160937](.\image-ai\屏幕截图 2025-02-11 160937.png)

![屏幕截图 2025-02-11 161045](.\image-ai\屏幕截图 2025-02-11 161045.png)

**文字表示**

![屏幕截图 2025-02-11 161251](.\image-ai\屏幕截图 2025-02-11 161251.png)

## 表示学习与嵌入

表示学习（Representation Learning）和嵌入（Embedding）是密切相关的概念，它们可以被视为在不同领域中对同一概念的不同命名或描述。

**表示学习**

通过学习算法**自动地**从原始数据中学习到一种**表示形式或特征表示**，该表示形式能够更好地表达数据的重要特征和结构。表示学习的目标是将输入数据转换为具有良好表示能力的特征空间，使得在该空间中的数据具有更好的可分性、可解释性或推理能力。

**嵌入**

表示学习的一种形式，通常用于将**高维数据映射到低维空间中**的表示形式。嵌入可以是词嵌入、图像嵌入、图嵌入等。例如，在自然语言处理中，词嵌入将词语映射到低维向量空间，以捕捉词语之间的语义和句法关系。在图像处理中，图像嵌入将图像映射到低维向量空间，以表示图像的视觉特征。

因此，嵌入可以被视为一种表示学习的特定形式，旨在将高维数据转换为低维向量表示。表示学习可以涉及更广泛的概念和方法，包括嵌入在内，以实现对数据的更好理解和表达。

![屏幕截图 2025-02-11 161534](.\image-ai\屏幕截图 2025-02-11 161534.png)

## 表示学习怎么学？

### 向量数据

向量数据是一种数学表示，用一组有序的数值表示一个对象或数据点。在机器学习中，向量可以表示诸如单词、图像、视频和音频之类的复杂对象，由机器学习（ML）模型生成。高维度的向量数据对于机器学习、自然语言处理（NLP）和其他人工智能任务至关重要。

向量数据库专门设计来存储高维向量数据。向量数据库使用嵌入模型将数据（如文本、图像等）转换为数值向量，能捕捉数据的语义或结构信息，便于进行高效的比较和相似性搜索。

### 表示学习:让计算机更好地理解世界

**表示学习（Representation Learning）**是指通过学习算法**自动地**从原始数据中学习到一种表示形式或特征表示，该表示形式能够更好地表达数据的重要特征和结构。表示学习的目标是将输入数据转换为具有良好表示能力的**特征空间**，使得在该空间中的数据具有更好的可分性、可解释性或推理能力。

- **可分性：**表示学习的目标之一是将输入数据转换为具有更好可分性的特征空间。这意味着在学习到的表示中，**不同类别或不同概念之间的样本应该有明显的边界或区别**。例如，在图像分类任务中，表示学习的目标是使来自不同类别的图像在特征空间中更容易区分。这样的特征表示使得机器学习算法可以更轻松地进行分类、聚类或其他数据分析任务。
- **可解释性：**另一个表示学习的目标是生成可解释性的特征表示。这意味着学习到的特征应该具有对应于原始数据中的可理解概念或语义的含义。例如，在自然语言处理中，词嵌入模型学习到的词向量应该能够捕捉到**词语之间的语义关系**，使得相似的词在向量空间中更接近。这样的表示不仅有助于模型的解释性，也可以在语义分析和文本生成等任务中提供更好的性能。
- **推理能力：**另一个重要的目标是使得学习到的特征表示在推理任务中更具能力。这意味着在特征空间中，我们可以执行类似于**推理、类比或关联**的操作。例如，通过在词嵌入空间中执行向量运算，如 "国王" - "男人" + "女人"，我们可以得到与 "皇后" 非常接近的结果。这种推理能力使得我们能够进行关联和类比推理，从而在自然语言处理、推荐系统和知识图谱等领域中实现更高级的语义推理和理解。

### 深度神经网络(DNN)：一种高效学习数据抽象特征表示的方法

深度神经网络是一种模拟人脑神经网络结构和功能的计算模型，是人工智能领域中的一种重要模型，它通过模拟人脑神经网络的结构和工作原理，实现了对复杂数据的处理和决策。

## 如何评估表示学习

### 嵌入（Embedding）的价值

![屏幕截图 2025-02-11 161534](.\image-ai\屏幕截图 2025-02-11 161534.png)

- **降维：**在许多实际问题中，原始数据的维度往往非常高。例如，在自然语言处理中，如果使用One-hot编码来表示词汇，其维度等于词汇表的大小，可能达到数十万甚至更高。通过Embedding，我们可以将这些高维数据映射到一个低维空间，大大减少了模型的复杂度。
- **捕捉语义信息：** Embedding不仅仅是降维，更重要的是，它能够捕捉到数据的语义信息。例如，在词嵌入中，语义上相近的词在向量空间中也会相近。这意味着Embedding可以保留并利用原始数据的一些重要信息。
- **适应性：** 与一些传统的特征提取方法相比，Embedding是通过数据驱动的方式学习的。这意味着它能够自动适应数据的特性，而无需人工设计特征。
- **泛化能力：** 在实际问题中，我们经常需要处理一些在训练数据中没有出现过的数据。由于Embedding能够捕捉到数据的一些内在规律，因此对于这些未见过的数据，Embedding仍然能够给出合理的表示。
- **可解释性：** 尽管Embedding是高维的，但我们可以通过一些可视化工具（如t-SNE）来观察和理解Embedding的结构。这对于理解模型的行为，以及发现数据的一些潜在规律是非常有用的。

### 嵌入（Embedding）

**嵌入（Embedding）**是表示学习的一种特定形式，旨在将高维数据映射到低维空间中的向量表示。

- **词嵌入（Word Embedding）：**在自然语言处理中，词嵌入将词语映射到低维向量空间，以捕捉词语之间的**语义和句法关系**。 通过学习词嵌入，可以将词语表示为连续的向量，其中相似的词语在向量空间中彼此靠近。它在自然语言处理任务中广泛应用，包括**词语相似度计算、文本分类、命名实体识别**等。词嵌入可以通过Word2Vec、GloVe 等方法进行学习。
- **图像嵌入（Image Embedding）**：在图像处理中，图像嵌入将图像映射到低维向量空间，以表示图像的**视觉特征**。这种嵌入方法通常通过使用卷积神经网络（Convolutional Neural Networks, CNN）等深度学习技术来提取图像的特征表示。
- **图嵌入（Graph Embedding）：**是用于学习图结构的表示学习方法，将图中的节点和边映射到低维向量空间中。通过学习图嵌入，可以将复杂的图结构转化为向量表示，**以捕捉节点之间的结构和关联关系**。这些方法可以通过DeepWalk、Node2Vec、GraphSAGE等算法来实现。图嵌入在图分析、社交网络分析、推荐系统等领域中广泛应用，用于发现社区结构、节点相似性、信息传播等图属性。

**包括嵌入在内，表示学习涉及更广泛的概念和方法，以实现对数据的更好理解和表达。**

![屏幕截图 2025-02-11 165749](.\image-ai\屏幕截图 2025-02-11 165749.png)

![屏幕截图 2025-02-11 170003](.\image-ai\屏幕截图 2025-02-11 170003.png)

![屏幕截图 2025-02-11 170039](.\image-ai\屏幕截图 2025-02-11 170039.png)

### Word Embedding

Word Embedding 为自然语言处理任务提供了更好的单词表示方法，它的应用主要有：

- **语义表示和语义相似度**：Word Embedding可以捕捉到单词之间的语义关系，使得相似含义的单词在向量空间中彼此靠近。这使得我们可以通过计算向量之间的距离或相似度来度量单词的语义相似度。这对于词义消歧、语义搜索、语义推理等任务非常有用。
- **词语关系和类比推理**：Word Embedding可以捕捉到单词之间的关系，如同义词、反义词、上下位关系等。通过在向量空间中进行向量运算，例如求解向量之间的差异或类比推理，我们可以发现词语之间的有趣关系。例如，对于词语之间的类比关系"king - man + woman = queen"，通过Word Embedding可以得到类似的结果。
- **上下文理解**：Word Embedding可以帮助理解单词的上下文信息。通过学习单词在其周围上下文中的嵌入表示，我们可以捕捉到单词的语境信息，从而帮助解决语义歧义、词语消歧和指代消解等问题。
- **文本分类和情感分析**：Word Embedding可以提供丰富的词语特征表示，从而改善文本分类和情感分析任务的性能。通过将文本中的词语映射为嵌入向量，并将这些向量作为输入特征，我们可以在分类器或情感分析模型中进行训练和预测。
-  **机器翻译和生成模型**：Word Embedding对于机器翻译和生成模型也是至关重要的。通过将源语言和目标语言的单词都映射为嵌入向量，可以提高翻译的准确性和生成模型的质量。嵌入向量可以捕捉到单词之间的语义和句法关系，帮助模型更好地理解和生成自然语言。

**Word Embedding vs Language Model**

- **Word Embedding：**词嵌入通常被用来生成词的向量表示，这个过程通常是**静态**的，即一旦训练完成，每个词的向量表示就确定了。词嵌入的主要目标是**捕获单词或短语的语义和语法信息**，并将这些信息以向量形式表示出来。词嵌入的一个重要特性是，语义上相近的词在嵌入空间中的距离也比较近。然而，词嵌入并不能理解上下文信息，即相同的词在不同的上下文中可能有不同的含义，但词嵌入无法区分这些含义。
- **Language Model：**语言模型则是**预测词序列的概率模型**，这个过程通常是**动态**的，会根据输入的上下文进行变化。语言模型的主要目标是**理解和生成文本**。这包括对上下文的理解，词的预测，句子的生成等等。语言模型会用到词嵌入，但同时也会对上下文进行建模，这样可以处理词在不同上下文中的不同含义。

在某种程度上，你可以将词嵌入看作是语言模型的一部分或者输入，语言模型使用词嵌入捕捉的信息，来进行更深层次的语义理解和文本生成。当然，现在有一些更先进的模型，比如BERT，GPT等，它们生成的是上下文相关的词嵌入，即词的嵌入会根据上下文变化，这样一定程度上弥补了传统词嵌入模型的不足。

![屏幕截图 2025-02-11 172349](.\image-ai\屏幕截图 2025-02-11 172349.png)

# OpenAI大模型开发与应用实践

## OpenAI 大模型开发指南